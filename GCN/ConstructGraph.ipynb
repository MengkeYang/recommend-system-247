{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "import pandas as pd\n",
    "import dgl.function as fn\n",
    "from dgl import DGLGraph\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from dgl.nn.pytorch import GraphConv\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Big Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_phase=6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "click_data=genfromtxt(f'./underexpose_train/underexpose_train_click-{0}.csv', delimiter=',',dtype=np.int32)\n",
    "for i in range(1,curr_phase+1):\n",
    "    click_data=np.concatenate((click_data, genfromtxt(f'./underexpose_train/underexpose_train_click-{i}.csv', delimiter=',',dtype=np.int32)))\n",
    "click_data=click_data[:,:2]\n",
    "users=click_data[:,0]\n",
    "items=click_data[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_vertices=len(np.unique(users))+len(np.unique(items))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "uid2vid={}\n",
    "vid2uid={}\n",
    "iid2vid={}\n",
    "vid2iid={}\n",
    "inc=0\n",
    "for user in np.unique(users):\n",
    "    uid2vid[user]=inc\n",
    "    vid2uid[inc]=user\n",
    "    inc+=1\n",
    "for item in np.unique(items):\n",
    "    iid2vid[item]=inc\n",
    "    vid2iid[inc]=item\n",
    "    inc+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert((len(iid2vid)+len(uid2vid))==total_vertices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "g=dgl.DGLGraph()\n",
    "g.add_nodes(total_vertices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(users)):\n",
    "    src=uid2vid[users[i]]\n",
    "    dst=iid2vid[items[i]]\n",
    "    g.add_edge(src,dst)\n",
    "    g.add_edge(dst,src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "g=dgl.transform.add_self_loop(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    " n_edges = g.number_of_edges()\n",
    "# normalization\n",
    "degs = g.in_degrees().float()\n",
    "norm = torch.pow(degs, -0.5)\n",
    "norm[torch.isinf(norm)] = 0\n",
    "if cuda:\n",
    "    norm = norm.cuda()\n",
    "g.ndata['norm'] = norm.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(29)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(g.out_degrees(np.arange(0,g.number_of_nodes())))/g.number_of_nodes()#average degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(g.in_degrees(np.arange(0,g.number_of_nodes())))[int(g.number_of_nodes()*0.2)] \n",
    "#what is the degree of 20 percentile of the lowest cliked items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DGLGraph(num_nodes=129813, num_edges=3843087,\n",
       "         ndata_schemes={'norm': Scheme(shape=(1,), dtype=torch.float32)}\n",
       "         edata_schemes={})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get User Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./underexpose_train/user_generate_feat.txt\",\"r\")as f:\n",
    "    lines=f.readlines()\n",
    "    usr_feat=np.zeros((len(lines),5))\n",
    "    for i in range(len(lines)):\n",
    "        if lines[i].split(\",\")[2]==\"0\":\n",
    "            usr_feat[i][3]=1\n",
    "        if lines[i].split(\",\")[2]==\"1\":\n",
    "            usr_feat[i][4]=1\n",
    "    del lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"./underexpose_train/underexpose_user_feat.csv\",\"r\")as f:\n",
    "#     lines=f.readlines()\n",
    "#     usr_feat=np.zeros((len(lines),5))\n",
    "#     for i in range(len(lines)):\n",
    "#         if lines[i].split(\",\")[2]==\"M\":\n",
    "#             usr_feat[i][3]=1\n",
    "#         if lines[i].split(\",\")[2]==\"F\":\n",
    "#             usr_feat[i][4]=1\n",
    "#     del lines\n",
    "# fn=\"./underexpose_train/underexpose_user_feat.csv\"\n",
    "# usr_data=genfromtxt(fn, delimiter=',',dtype=np.int16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn=\"./underexpose_train/user_generate_feat.txt\"\n",
    "usr_data=genfromtxt(fn, delimiter=',',dtype=np.int16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "usr_feat[:,0:2]=usr_data[:,0:2]\n",
    "usr_feat[:,2]=usr_data[:,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "uid2feat={}\n",
    "for i in range(len(usr_feat)):\n",
    "    uid2feat[int(usr_feat[i][0])]=usr_feat[i][1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "del usr_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Item Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_data=genfromtxt(\"./underexpose_train/underexpose_item_feat_clean.csv\", delimiter=',',dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_ids=item_data[:,0].astype(np.int)\n",
    "item_feats=item_data[:,1:].astype(np.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "iid2feat={}\n",
    "for i in range(len(item_ids)):\n",
    "    iid2feat[item_ids[i]]=item_feats[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "del item_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Generate Entire Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each vertex (either USER or ITEM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "ufeatdim=uid2feat[17].shape[0]\n",
    "ifeatdim=iid2feat[2000].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "features=np.zeros((total_vertices,ufeatdim+ifeatdim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureless_cnt=0\n",
    "for i in range(total_vertices):\n",
    "    if i in vid2uid and vid2uid[i] in uid2feat:\n",
    "        features[i,0:ufeatdim] = uid2feat[vid2uid[i]]\n",
    "        continue\n",
    "    if i in vid2iid and vid2iid[i] in iid2feat:\n",
    "        features[i,ufeatdim:] = iid2feat[vid2iid[i]]\n",
    "        continue\n",
    "    featureless_cnt+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(129813, 260)"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(features/np.max(features,axis=0)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09240985109349603"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featureless_cnt/total_vertices #~25% node are feature less ||should I do an average of neighors of neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = torch.FloatTensor(features/np.max(features,axis=0)) #normalize each feature\n",
    "if cuda:\n",
    "    features = features.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Model GCN +BCELoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(nn.Module):\n",
    "    def __init__(self,\n",
    "                 g,\n",
    "                 in_feats,\n",
    "                 n_hidden,\n",
    "                 n_classes,\n",
    "                 n_layers,\n",
    "                 activation,\n",
    "                 dropout):\n",
    "        super(GCN, self).__init__()\n",
    "        self.g = g\n",
    "        self.layers = nn.ModuleList()\n",
    "        # input layer\n",
    "        self.layers.append(GraphConv(in_feats, n_hidden, activation=activation))\n",
    "        # hidden layers\n",
    "        for i in range(n_layers - 1):\n",
    "            self.layers.append(GraphConv(n_hidden, n_hidden, activation=activation))\n",
    "        # output layer\n",
    "        self.layers.append(GraphConv(n_hidden, n_classes))\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.loss_fn = nn.BCELoss() \n",
    "        \n",
    "    def forward(self, features):\n",
    "        h = features\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            if i != 0:\n",
    "                h = self.dropout(h)\n",
    "            h = layer(self.g, h)\n",
    "        return h;\n",
    "    \n",
    "    def loss(self, h, pos_usrs,pos_itms,neg_itms):\n",
    "        #h_i dot h_j\n",
    "\n",
    "        pos_usr_embedding=h[pos_usrs,:]\n",
    "        pos_itm_embedding=h[pos_itms,:]\n",
    "        pos_pred=pos_itm_embedding*pos_usr_embedding\n",
    "        pos_pred= torch.sum(pos_pred,dim=1)\n",
    "        pos_pred= torch.sigmoid(pos_pred)\n",
    "        correct = torch.sum(pos_pred>0.5)\n",
    "        pred = pos_pred\n",
    "        for i in range(len(neg_itms)):\n",
    "            neg_itm_embedding=h[neg_itms[i],:]\n",
    "            neg_pred = torch.mul(neg_itm_embedding,pos_usr_embedding[i])\n",
    "            neg_pred= torch.sum(neg_pred,dim=1)\n",
    "            neg_pred = torch.sigmoid(neg_pred)\n",
    "            pred = torch.cat((pred, neg_pred),0)\n",
    "            correct += torch.sum(neg_pred<=.5)\n",
    "        \n",
    "        print(\"Training Accuracy: \", end =\"\")\n",
    "        print(float(correct/float(pos_usrs.shape[0]+neg_itms.shape[0]*neg_itms.shape[1])))\n",
    "            \n",
    "        label = torch.cat((torch.ones(pos_usrs.shape[0]),torch.zeros(neg_itms.shape[0]*neg_itms.shape[1])),0).cuda();\n",
    "        return self.loss_fn(pred,label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=GCN(g,features.shape[1],512,256,1,F.relu,0.2)\n",
    "if cuda:\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees=g.in_degrees(np.arange(0,g.number_of_nodes()))\n",
    "prob=np.power(degrees,0.75)/float(torch.sum(np.power(degrees,0.75)))\n",
    "\n",
    "def sample_neg(pos_usr, neg_size):\n",
    "    \"\"\"\n",
    "        For generating batches of data.\n",
    "    \"\"\"\n",
    "    samples = np.random.choice(len(degrees),(len(pos_usr),neg_size),p=prob)\n",
    "    for i in range(samples.shape[0]):\n",
    "        for j in range(samples.shape[1]):\n",
    "            while g.has_edge_between(pos_usr[i],samples[i][j]):\n",
    "                samples[i][j]=np.random.choice(len(degrees), 1 ,p=prob)\n",
    "    return samples\n",
    "\n",
    "def check_accuracy(h,  pos_usrs,pos_itms,neg_itms):#check random n links\n",
    "    with torch.no_grad():\n",
    "        pos_usr_embedding=h[pos_usrs,:]\n",
    "        pos_itm_embedding=h[pos_itms,:]\n",
    "        pos_pred=pos_itm_embedding*pos_usr_embedding\n",
    "        pos_pred= torch.sum(pos_pred,dim=1)\n",
    "        pos_pred= torch.sigmoid(pos_pred)\n",
    "        correct = torch.sum(pos_pred>0.5)\n",
    "        \n",
    "        for i in range(len(neg_itms)):\n",
    "            neg_itm_embedding=h[neg_itms[i],:]\n",
    "            neg_pred = torch.mul(neg_itm_embedding,pos_usr_embedding[i])\n",
    "            neg_pred= torch.sum(neg_pred,dim=1)\n",
    "            neg_pred = torch.sigmoid(neg_pred)\n",
    "            correct += torch.sum(neg_pred<=.5)\n",
    "        return correct/float(pos_usrs.shape[0]+neg_itms.shape[0]*neg_itms.shape[1])\n",
    "       \n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.5\n",
      "Loss: 3.0833377838134766\n",
      "Accuracy: 0.5\n",
      "Training Accuracy: 0.5\n",
      "Loss: 1.5526411533355713\n",
      "Training Accuracy: 0.4998333156108856\n",
      "Loss: 0.9208746552467346\n",
      "Training Accuracy: 0.500166654586792\n",
      "Loss: 0.7669858336448669\n",
      "Training Accuracy: 0.5\n",
      "Loss: 0.7807765007019043\n",
      "Training Accuracy: 0.5\n",
      "Loss: 0.8290231227874756\n",
      "Training Accuracy: 0.5\n",
      "Loss: 0.822573721408844\n",
      "Training Accuracy: 0.5\n",
      "Loss: 0.789300799369812\n",
      "Training Accuracy: 0.5\n",
      "Loss: 0.7554960250854492\n",
      "Training Accuracy: 0.5\n",
      "Loss: 0.7358836531639099\n",
      "Training Accuracy: 0.5\n",
      "Loss: 0.720391035079956\n",
      "Accuracy: 0.5\n",
      "Training Accuracy: 0.500166654586792\n",
      "Loss: 0.7122898697853088\n",
      "Training Accuracy: 0.500333309173584\n",
      "Loss: 0.7012906670570374\n",
      "Training Accuracy: 0.5005000233650208\n",
      "Loss: 0.6975200176239014\n",
      "Training Accuracy: 0.5006666779518127\n",
      "Loss: 0.6923235654830933\n",
      "Training Accuracy: 0.5011666417121887\n",
      "Loss: 0.6871898770332336\n",
      "Training Accuracy: 0.5013333559036255\n",
      "Loss: 0.6831796765327454\n",
      "Training Accuracy: 0.5021666884422302\n",
      "Loss: 0.6816001534461975\n",
      "Training Accuracy: 0.5013333559036255\n",
      "Loss: 0.6774202585220337\n",
      "Training Accuracy: 0.5021666884422302\n",
      "Loss: 0.6724964380264282\n",
      "Training Accuracy: 0.5036666393280029\n",
      "Loss: 0.6710944175720215\n",
      "Accuracy: 0.503250002861023\n",
      "Training Accuracy: 0.5048333406448364\n",
      "Loss: 0.6649389863014221\n",
      "Training Accuracy: 0.5061666369438171\n",
      "Loss: 0.6631794571876526\n",
      "Training Accuracy: 0.5056666731834412\n",
      "Loss: 0.6613039374351501\n",
      "Training Accuracy: 0.5116666555404663\n",
      "Loss: 0.6594803929328918\n",
      "Training Accuracy: 0.5118333101272583\n",
      "Loss: 0.6541540622711182\n",
      "Training Accuracy: 0.5105000138282776\n",
      "Loss: 0.6536163091659546\n",
      "Training Accuracy: 0.5128333568572998\n",
      "Loss: 0.6508138179779053\n",
      "Training Accuracy: 0.5134999752044678\n",
      "Loss: 0.6522715091705322\n",
      "Training Accuracy: 0.5223333239555359\n",
      "Loss: 0.6482621431350708\n",
      "Training Accuracy: 0.5263333320617676\n",
      "Loss: 0.6462436318397522\n",
      "Accuracy: 0.5273750424385071\n",
      "Training Accuracy: 0.5303333401679993\n",
      "Loss: 0.6464151740074158\n",
      "Training Accuracy: 0.5339999794960022\n",
      "Loss: 0.6399615406990051\n",
      "Training Accuracy: 0.528166651725769\n",
      "Loss: 0.644149661064148\n",
      "Training Accuracy: 0.531000018119812\n",
      "Loss: 0.6422768831253052\n",
      "Training Accuracy: 0.5376666784286499\n",
      "Loss: 0.6469969749450684\n",
      "Training Accuracy: 0.5419999957084656\n",
      "Loss: 0.6392199993133545\n",
      "Training Accuracy: 0.5425000190734863\n",
      "Loss: 0.6371943354606628\n",
      "Training Accuracy: 0.5456666350364685\n",
      "Loss: 0.6379613876342773\n",
      "Training Accuracy: 0.5403333306312561\n",
      "Loss: 0.6365966200828552\n",
      "Training Accuracy: 0.5458333492279053\n",
      "Loss: 0.6315048933029175\n",
      "Accuracy: 0.5444375276565552\n",
      "Training Accuracy: 0.5496666431427002\n",
      "Loss: 0.6371614336967468\n",
      "Training Accuracy: 0.5533333420753479\n",
      "Loss: 0.63639235496521\n",
      "Training Accuracy: 0.5598333477973938\n",
      "Loss: 0.6331421136856079\n",
      "Training Accuracy: 0.5646666884422302\n",
      "Loss: 0.6348414421081543\n",
      "Training Accuracy: 0.565500020980835\n",
      "Loss: 0.634810209274292\n",
      "Training Accuracy: 0.5711666345596313\n",
      "Loss: 0.6322523355484009\n",
      "Training Accuracy: 0.5721666812896729\n",
      "Loss: 0.6304746270179749\n",
      "Training Accuracy: 0.5696666836738586\n",
      "Loss: 0.6298792958259583\n",
      "Training Accuracy: 0.5799999833106995\n",
      "Loss: 0.626242458820343\n",
      "Training Accuracy: 0.577833354473114\n",
      "Loss: 0.6316533088684082\n",
      "Accuracy: 0.5720000267028809\n",
      "Training Accuracy: 0.5806666612625122\n",
      "Loss: 0.6280925273895264\n",
      "Training Accuracy: 0.5809999704360962\n",
      "Loss: 0.6308557987213135\n",
      "Training Accuracy: 0.5851666331291199\n",
      "Loss: 0.633832573890686\n",
      "Training Accuracy: 0.5806666612625122\n",
      "Loss: 0.6321223974227905\n",
      "Training Accuracy: 0.593833327293396\n",
      "Loss: 0.6246552467346191\n",
      "Training Accuracy: 0.5876666307449341\n",
      "Loss: 0.6312167048454285\n",
      "Training Accuracy: 0.5929999947547913\n",
      "Loss: 0.6251599788665771\n",
      "Training Accuracy: 0.5973333120346069\n",
      "Loss: 0.6275354027748108\n",
      "Training Accuracy: 0.5988333225250244\n",
      "Loss: 0.6298086047172546\n",
      "Training Accuracy: 0.609499990940094\n",
      "Loss: 0.6233615279197693\n",
      "Accuracy: 0.6061250567436218\n",
      "Training Accuracy: 0.6083333492279053\n",
      "Loss: 0.625869870185852\n",
      "Training Accuracy: 0.6121666431427002\n",
      "Loss: 0.6220982074737549\n",
      "Training Accuracy: 0.6100000143051147\n",
      "Loss: 0.622170627117157\n",
      "Training Accuracy: 0.6119999885559082\n",
      "Loss: 0.6240073442459106\n",
      "Training Accuracy: 0.6243333220481873\n",
      "Loss: 0.6182323694229126\n",
      "Training Accuracy: 0.622166633605957\n",
      "Loss: 0.6205399036407471\n",
      "Training Accuracy: 0.6193333268165588\n",
      "Loss: 0.6204166412353516\n",
      "Training Accuracy: 0.6128333210945129\n",
      "Loss: 0.6179664134979248\n",
      "Training Accuracy: 0.6173333525657654\n",
      "Loss: 0.6191214323043823\n",
      "Training Accuracy: 0.6238332986831665\n",
      "Loss: 0.6140982508659363\n",
      "Accuracy: 0.6216875314712524\n",
      "Training Accuracy: 0.6228333115577698\n",
      "Loss: 0.6205108761787415\n",
      "Training Accuracy: 0.6276666522026062\n",
      "Loss: 0.618726372718811\n",
      "Training Accuracy: 0.6238332986831665\n",
      "Loss: 0.6132096648216248\n",
      "Training Accuracy: 0.6233333349227905\n",
      "Loss: 0.615498960018158\n",
      "Training Accuracy: 0.6166666746139526\n",
      "Loss: 0.6141121983528137\n",
      "Training Accuracy: 0.6234999895095825\n",
      "Loss: 0.6136351823806763\n",
      "Training Accuracy: 0.6301666498184204\n",
      "Loss: 0.6109889149665833\n",
      "Training Accuracy: 0.6340000033378601\n",
      "Loss: 0.6223719716072083\n",
      "Training Accuracy: 0.6368333101272583\n",
      "Loss: 0.615939736366272\n",
      "Training Accuracy: 0.6321666836738586\n",
      "Loss: 0.6142956614494324\n",
      "Accuracy: 0.6356250047683716\n",
      "Training Accuracy: 0.6194999814033508\n",
      "Loss: 0.6121797561645508\n",
      "Training Accuracy: 0.6234999895095825\n",
      "Loss: 0.6048183441162109\n",
      "Training Accuracy: 0.6316666603088379\n",
      "Loss: 0.6060203909873962\n",
      "Training Accuracy: 0.6291666626930237\n",
      "Loss: 0.6085805892944336\n",
      "Training Accuracy: 0.6426666378974915\n",
      "Loss: 0.6082758903503418\n",
      "Training Accuracy: 0.6456666588783264\n",
      "Loss: 0.6068253517150879\n",
      "Training Accuracy: 0.6423333287239075\n",
      "Loss: 0.6068313717842102\n",
      "Training Accuracy: 0.6340000033378601\n",
      "Loss: 0.6076321005821228\n",
      "Training Accuracy: 0.6331666707992554\n",
      "Loss: 0.6050257682800293\n",
      "Training Accuracy: 0.6368333101272583\n",
      "Loss: 0.6003453135490417\n",
      "Accuracy: 0.6382500529289246\n",
      "Training Accuracy: 0.640333354473114\n",
      "Loss: 0.6070313453674316\n",
      "Training Accuracy: 0.6456666588783264\n",
      "Loss: 0.6082982420921326\n",
      "Training Accuracy: 0.6418333053588867\n",
      "Loss: 0.6108825206756592\n",
      "Training Accuracy: 0.6464999914169312\n",
      "Loss: 0.6105819344520569\n",
      "Training Accuracy: 0.643833339214325\n",
      "Loss: 0.6048532724380493\n",
      "Training Accuracy: 0.6349999904632568\n",
      "Loss: 0.6024025082588196\n",
      "Training Accuracy: 0.6306666731834412\n",
      "Loss: 0.6051779985427856\n",
      "Training Accuracy: 0.6305000185966492\n",
      "Loss: 0.6109829545021057\n",
      "Training Accuracy: 0.6421666741371155\n",
      "Loss: 0.6064432859420776\n",
      "Training Accuracy: 0.6445000171661377\n",
      "Loss: 0.6032599210739136\n",
      "Accuracy: 0.6491250395774841\n",
      "Training Accuracy: 0.6481666564941406\n",
      "Loss: 0.6011589169502258\n",
      "Training Accuracy: 0.6426666378974915\n",
      "Loss: 0.5999954342842102\n",
      "Training Accuracy: 0.6418333053588867\n",
      "Loss: 0.6016054153442383\n",
      "Training Accuracy: 0.6391666531562805\n",
      "Loss: 0.5987002849578857\n",
      "Training Accuracy: 0.6451666355133057\n",
      "Loss: 0.5985285639762878\n",
      "Training Accuracy: 0.640500009059906\n",
      "Loss: 0.5985332727432251\n",
      "Training Accuracy: 0.6484999656677246\n",
      "Loss: 0.5996466279029846\n",
      "Training Accuracy: 0.6495000123977661\n",
      "Loss: 0.6013139486312866\n",
      "Training Accuracy: 0.6446666717529297\n",
      "Loss: 0.596800684928894\n",
      "Training Accuracy: 0.652999997138977\n",
      "Loss: 0.6008051633834839\n",
      "Accuracy: 0.6539375185966492\n",
      "Training Accuracy: 0.6513333320617676\n",
      "Loss: 0.5976679921150208\n",
      "Training Accuracy: 0.6520000100135803\n",
      "Loss: 0.5983355045318604\n",
      "Training Accuracy: 0.6456666588783264\n",
      "Loss: 0.5908229947090149\n",
      "Training Accuracy: 0.637666642665863\n",
      "Loss: 0.5992777943611145\n",
      "Training Accuracy: 0.6430000066757202\n",
      "Loss: 0.5965268611907959\n",
      "Training Accuracy: 0.6441666483879089\n",
      "Loss: 0.5982044339179993\n",
      "Training Accuracy: 0.6541666388511658\n",
      "Loss: 0.5947162508964539\n",
      "Training Accuracy: 0.6523333191871643\n",
      "Loss: 0.5891153812408447\n",
      "Training Accuracy: 0.6526666879653931\n",
      "Loss: 0.5934778451919556\n",
      "Training Accuracy: 0.6466666460037231\n",
      "Loss: 0.5984958410263062\n",
      "Accuracy: 0.6495625376701355\n",
      "Training Accuracy: 0.6455000042915344\n",
      "Loss: 0.5919259190559387\n",
      "Training Accuracy: 0.6536666750907898\n",
      "Loss: 0.5928143262863159\n",
      "Training Accuracy: 0.6608332991600037\n",
      "Loss: 0.5928769707679749\n",
      "Training Accuracy: 0.6608332991600037\n",
      "Loss: 0.5917949676513672\n",
      "Training Accuracy: 0.6628333330154419\n",
      "Loss: 0.5876251459121704\n",
      "Training Accuracy: 0.6546666622161865\n",
      "Loss: 0.5916343331336975\n",
      "Training Accuracy: 0.6646666526794434\n",
      "Loss: 0.5909788012504578\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.6588333249092102\n",
      "Loss: 0.591298520565033\n",
      "Training Accuracy: 0.6553333401679993\n",
      "Loss: 0.5929868817329407\n",
      "Training Accuracy: 0.6641666889190674\n",
      "Loss: 0.591706395149231\n",
      "Accuracy: 0.6594375371932983\n",
      "Training Accuracy: 0.6678333282470703\n",
      "Loss: 0.5916755199432373\n",
      "Training Accuracy: 0.6704999804496765\n",
      "Loss: 0.5852190852165222\n",
      "Training Accuracy: 0.6671666502952576\n",
      "Loss: 0.58699631690979\n",
      "Training Accuracy: 0.6520000100135803\n",
      "Loss: 0.5939818024635315\n",
      "Training Accuracy: 0.659333348274231\n",
      "Loss: 0.5898462533950806\n",
      "Training Accuracy: 0.6598333120346069\n",
      "Loss: 0.593115508556366\n",
      "Training Accuracy: 0.668999969959259\n",
      "Loss: 0.5865436792373657\n",
      "Training Accuracy: 0.6696666479110718\n",
      "Loss: 0.5834317803382874\n",
      "Training Accuracy: 0.6623333096504211\n",
      "Loss: 0.5806114077568054\n",
      "Training Accuracy: 0.6679999828338623\n",
      "Loss: 0.5841295719146729\n",
      "Accuracy: 0.6598750352859497\n",
      "Training Accuracy: 0.6714999675750732\n",
      "Loss: 0.5823725461959839\n",
      "Training Accuracy: 0.6781666874885559\n",
      "Loss: 0.5802643299102783\n",
      "Training Accuracy: 0.6711666584014893\n",
      "Loss: 0.5839365124702454\n",
      "Training Accuracy: 0.6741666793823242\n",
      "Loss: 0.5800572037696838\n",
      "Training Accuracy: 0.6704999804496765\n",
      "Loss: 0.580899715423584\n",
      "Training Accuracy: 0.6759999990463257\n",
      "Loss: 0.580256998538971\n",
      "Training Accuracy: 0.6731666326522827\n",
      "Loss: 0.5894719958305359\n",
      "Training Accuracy: 0.675166666507721\n",
      "Loss: 0.5828902125358582\n",
      "Training Accuracy: 0.6766666769981384\n",
      "Loss: 0.5726944804191589\n",
      "Training Accuracy: 0.6666666865348816\n",
      "Loss: 0.5868375897407532\n",
      "Accuracy: 0.6693125367164612\n",
      "Training Accuracy: 0.671999990940094\n",
      "Loss: 0.5828935503959656\n",
      "Training Accuracy: 0.6765000224113464\n",
      "Loss: 0.5782127380371094\n",
      "Training Accuracy: 0.6753333210945129\n",
      "Loss: 0.5784148573875427\n",
      "Training Accuracy: 0.6806666851043701\n",
      "Loss: 0.5828913450241089\n",
      "Training Accuracy: 0.6756666302680969\n",
      "Loss: 0.5808486938476562\n",
      "Training Accuracy: 0.6826666593551636\n",
      "Loss: 0.5754601955413818\n",
      "Training Accuracy: 0.6775000095367432\n",
      "Loss: 0.5771203637123108\n",
      "Training Accuracy: 0.6786666512489319\n",
      "Loss: 0.5718889236450195\n",
      "Training Accuracy: 0.6759999990463257\n",
      "Loss: 0.5769453644752502\n",
      "Training Accuracy: 0.6803333163261414\n",
      "Loss: 0.579527735710144\n",
      "Accuracy: 0.6786250472068787\n",
      "Training Accuracy: 0.6821666359901428\n",
      "Loss: 0.5737422108650208\n",
      "Training Accuracy: 0.6833333373069763\n",
      "Loss: 0.578707218170166\n",
      "Training Accuracy: 0.6769999861717224\n",
      "Loss: 0.5739861726760864\n",
      "Training Accuracy: 0.6848333477973938\n",
      "Loss: 0.5711071491241455\n",
      "Training Accuracy: 0.6838333010673523\n",
      "Loss: 0.5767248272895813\n",
      "Training Accuracy: 0.6909999847412109\n",
      "Loss: 0.5834376811981201\n",
      "Training Accuracy: 0.6923333406448364\n",
      "Loss: 0.5666676163673401\n",
      "Training Accuracy: 0.6840000152587891\n",
      "Loss: 0.5754277110099792\n",
      "Training Accuracy: 0.6840000152587891\n",
      "Loss: 0.5747065544128418\n",
      "Training Accuracy: 0.6858333349227905\n",
      "Loss: 0.5691920518875122\n",
      "Accuracy: 0.6756875514984131\n",
      "Training Accuracy: 0.6806666851043701\n",
      "Loss: 0.5724741816520691\n",
      "Training Accuracy: 0.6821666359901428\n",
      "Loss: 0.5767889022827148\n",
      "Training Accuracy: 0.6868333220481873\n",
      "Loss: 0.5754044055938721\n",
      "Training Accuracy: 0.6896666884422302\n",
      "Loss: 0.5674328804016113\n",
      "Training Accuracy: 0.6875\n",
      "Loss: 0.5692163705825806\n",
      "Training Accuracy: 0.6768333315849304\n",
      "Loss: 0.5732004642486572\n",
      "Training Accuracy: 0.6886666417121887\n",
      "Loss: 0.5732553005218506\n",
      "Training Accuracy: 0.6894999742507935\n",
      "Loss: 0.5684229731559753\n",
      "Training Accuracy: 0.6861666440963745\n",
      "Loss: 0.5685423612594604\n",
      "Training Accuracy: 0.6808333396911621\n",
      "Loss: 0.569214940071106\n",
      "Accuracy: 0.6941250562667847\n",
      "Training Accuracy: 0.6883333325386047\n",
      "Loss: 0.5662490129470825\n",
      "Training Accuracy: 0.687333345413208\n",
      "Loss: 0.5748804807662964\n",
      "Training Accuracy: 0.6869999766349792\n",
      "Loss: 0.5706292986869812\n",
      "Training Accuracy: 0.6909999847412109\n",
      "Loss: 0.5671800971031189\n",
      "Training Accuracy: 0.6869999766349792\n",
      "Loss: 0.5640454888343811\n",
      "Training Accuracy: 0.6913333535194397\n",
      "Loss: 0.5616991519927979\n",
      "Training Accuracy: 0.6926666498184204\n",
      "Loss: 0.5655481815338135\n",
      "Training Accuracy: 0.6869999766349792\n",
      "Loss: 0.569226086139679\n",
      "Training Accuracy: 0.6918333172798157\n",
      "Loss: 0.5710771083831787\n",
      "Training Accuracy: 0.6953333020210266\n",
      "Loss: 0.5559049248695374\n",
      "Accuracy: 0.6910625100135803\n",
      "Training Accuracy: 0.6851666569709778\n",
      "Loss: 0.5723384618759155\n",
      "Training Accuracy: 0.690500020980835\n",
      "Loss: 0.5617468357086182\n",
      "Training Accuracy: 0.6899999976158142\n",
      "Loss: 0.5666784048080444\n",
      "Training Accuracy: 0.6948333382606506\n",
      "Loss: 0.563556969165802\n",
      "Training Accuracy: 0.6959999799728394\n",
      "Loss: 0.5597779154777527\n",
      "Training Accuracy: 0.6894999742507935\n",
      "Loss: 0.5669505596160889\n",
      "Training Accuracy: 0.6936666369438171\n",
      "Loss: 0.5667136907577515\n",
      "Training Accuracy: 0.6865000128746033\n",
      "Loss: 0.565459132194519\n",
      "Training Accuracy: 0.6868333220481873\n",
      "Loss: 0.5573104023933411\n",
      "Training Accuracy: 0.6944999694824219\n",
      "Loss: 0.5715397000312805\n",
      "Accuracy: 0.6938750147819519\n",
      "Training Accuracy: 0.6940000057220459\n",
      "Loss: 0.5635786652565002\n",
      "Training Accuracy: 0.687833309173584\n",
      "Loss: 0.5706871151924133\n",
      "Training Accuracy: 0.6919999718666077\n",
      "Loss: 0.5634222626686096\n",
      "Training Accuracy: 0.6891666650772095\n",
      "Loss: 0.5646882057189941\n",
      "Training Accuracy: 0.6934999823570251\n",
      "Loss: 0.5597423315048218\n",
      "Training Accuracy: 0.7026666402816772\n",
      "Loss: 0.5549460649490356\n",
      "Training Accuracy: 0.6976666450500488\n",
      "Loss: 0.557290256023407\n",
      "Training Accuracy: 0.6798333525657654\n",
      "Loss: 0.5655704140663147\n",
      "Training Accuracy: 0.6963333487510681\n",
      "Loss: 0.5568132400512695\n",
      "Training Accuracy: 0.6978332996368408\n",
      "Loss: 0.5661992430686951\n",
      "Accuracy: 0.6960000395774841\n",
      "Training Accuracy: 0.6980000138282776\n",
      "Loss: 0.5636267066001892\n",
      "Training Accuracy: 0.6940000057220459\n",
      "Loss: 0.5663878321647644\n",
      "Training Accuracy: 0.6938333511352539\n",
      "Loss: 0.5615778565406799\n",
      "Training Accuracy: 0.6875\n",
      "Loss: 0.560770571231842\n",
      "Training Accuracy: 0.7008333206176758\n",
      "Loss: 0.5597903728485107\n",
      "Training Accuracy: 0.7023333311080933\n",
      "Loss: 0.5619789361953735\n",
      "Training Accuracy: 0.700166642665863\n",
      "Loss: 0.5576483011245728\n",
      "Training Accuracy: 0.7048333287239075\n",
      "Loss: 0.5594698190689087\n",
      "Training Accuracy: 0.6933333277702332\n",
      "Loss: 0.5642087459564209\n",
      "Training Accuracy: 0.6916666626930237\n",
      "Loss: 0.5634855031967163\n",
      "Accuracy: 0.6993125081062317\n",
      "Training Accuracy: 0.6974999904632568\n",
      "Loss: 0.5561197400093079\n",
      "Training Accuracy: 0.6991666555404663\n",
      "Loss: 0.5641120672225952\n",
      "Training Accuracy: 0.6926666498184204\n",
      "Loss: 0.5665348768234253\n",
      "Training Accuracy: 0.699833333492279\n",
      "Loss: 0.5595937371253967\n",
      "Training Accuracy: 0.6991666555404663\n",
      "Loss: 0.563507080078125\n",
      "Training Accuracy: 0.703166663646698\n",
      "Loss: 0.5532728433609009\n",
      "Training Accuracy: 0.6933333277702332\n",
      "Loss: 0.551872730255127\n",
      "Training Accuracy: 0.6933333277702332\n",
      "Loss: 0.5533119440078735\n",
      "Training Accuracy: 0.7016666531562805\n",
      "Loss: 0.5554245710372925\n",
      "Training Accuracy: 0.6963333487510681\n",
      "Loss: 0.5599900484085083\n",
      "Accuracy: 0.6996875405311584\n",
      "Training Accuracy: 0.7006666660308838\n",
      "Loss: 0.5525814890861511\n",
      "Training Accuracy: 0.7023333311080933\n",
      "Loss: 0.5526049733161926\n",
      "Training Accuracy: 0.699833333492279\n",
      "Loss: 0.5587432384490967\n",
      "Training Accuracy: 0.6934999823570251\n",
      "Loss: 0.5635543465614319\n",
      "Training Accuracy: 0.7014999985694885\n",
      "Loss: 0.5512244701385498\n",
      "Training Accuracy: 0.7039999961853027\n",
      "Loss: 0.5533640384674072\n",
      "Training Accuracy: 0.6971666812896729\n",
      "Loss: 0.5596634745597839\n",
      "Training Accuracy: 0.6941666603088379\n",
      "Loss: 0.5577839612960815\n",
      "Training Accuracy: 0.7118332982063293\n",
      "Loss: 0.5535575747489929\n",
      "Training Accuracy: 0.7051666378974915\n",
      "Loss: 0.5517209768295288\n",
      "Accuracy: 0.702750027179718\n",
      "Training Accuracy: 0.7051666378974915\n",
      "Loss: 0.5483506917953491\n",
      "Training Accuracy: 0.6934999823570251\n",
      "Loss: 0.5561556220054626\n",
      "Training Accuracy: 0.6961666345596313\n",
      "Loss: 0.554780900478363\n",
      "Training Accuracy: 0.6956666707992554\n",
      "Loss: 0.55937659740448\n",
      "Training Accuracy: 0.7055000066757202\n",
      "Loss: 0.5480679273605347\n",
      "Training Accuracy: 0.7095000147819519\n",
      "Loss: 0.5580435395240784\n",
      "Training Accuracy: 0.6968333125114441\n",
      "Loss: 0.555118203163147\n",
      "Training Accuracy: 0.7043333053588867\n",
      "Loss: 0.5512903928756714\n",
      "Training Accuracy: 0.6991666555404663\n",
      "Loss: 0.5477066040039062\n",
      "Training Accuracy: 0.6981666684150696\n",
      "Loss: 0.5540663003921509\n",
      "Accuracy: 0.7009375095367432\n",
      "Training Accuracy: 0.7068333029747009\n",
      "Loss: 0.5476717948913574\n",
      "Training Accuracy: 0.7088333368301392\n",
      "Loss: 0.5427989363670349\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.70333331823349\n",
      "Loss: 0.5521110892295837\n",
      "Training Accuracy: 0.6978332996368408\n",
      "Loss: 0.5528357028961182\n",
      "Training Accuracy: 0.699833333492279\n",
      "Loss: 0.5468459725379944\n",
      "Training Accuracy: 0.6996666789054871\n",
      "Loss: 0.5545580387115479\n",
      "Training Accuracy: 0.7081666588783264\n",
      "Loss: 0.551224946975708\n",
      "Training Accuracy: 0.699999988079071\n",
      "Loss: 0.5461563467979431\n",
      "Training Accuracy: 0.7133333086967468\n",
      "Loss: 0.5389381647109985\n",
      "Training Accuracy: 0.7120000123977661\n",
      "Loss: 0.5446487665176392\n",
      "Accuracy: 0.7069375514984131\n",
      "Training Accuracy: 0.7066666483879089\n",
      "Loss: 0.5504110455513\n",
      "Training Accuracy: 0.702833354473114\n",
      "Loss: 0.5476747751235962\n",
      "Training Accuracy: 0.7133333086967468\n",
      "Loss: 0.5357396006584167\n",
      "Training Accuracy: 0.6990000009536743\n",
      "Loss: 0.5505207777023315\n",
      "Training Accuracy: 0.7023333311080933\n",
      "Loss: 0.5470067262649536\n",
      "Training Accuracy: 0.706166684627533\n",
      "Loss: 0.5408061742782593\n",
      "Training Accuracy: 0.7109999656677246\n",
      "Loss: 0.5473122596740723\n",
      "Training Accuracy: 0.7103333473205566\n",
      "Loss: 0.5500506162643433\n",
      "Training Accuracy: 0.7078333497047424\n",
      "Loss: 0.5487003326416016\n",
      "Training Accuracy: 0.7163333296775818\n",
      "Loss: 0.5420301556587219\n",
      "Accuracy: 0.7006250619888306\n",
      "Training Accuracy: 0.7105000019073486\n",
      "Loss: 0.5438557863235474\n",
      "Training Accuracy: 0.7195000052452087\n",
      "Loss: 0.5359146595001221\n",
      "Training Accuracy: 0.7074999809265137\n",
      "Loss: 0.5461624264717102\n",
      "Training Accuracy: 0.7045000195503235\n",
      "Loss: 0.5475350022315979\n",
      "Training Accuracy: 0.7148333191871643\n",
      "Loss: 0.5413243174552917\n",
      "Training Accuracy: 0.7011666297912598\n",
      "Loss: 0.5489501953125\n",
      "Training Accuracy: 0.7016666531562805\n",
      "Loss: 0.5557758212089539\n",
      "Training Accuracy: 0.7009999752044678\n",
      "Loss: 0.5440910458564758\n",
      "Training Accuracy: 0.6994999647140503\n",
      "Loss: 0.5449780821800232\n",
      "Training Accuracy: 0.699999988079071\n",
      "Loss: 0.5444848537445068\n",
      "Accuracy: 0.7116250395774841\n",
      "Training Accuracy: 0.7081666588783264\n",
      "Loss: 0.5434834957122803\n",
      "Training Accuracy: 0.7103333473205566\n",
      "Loss: 0.5439366102218628\n",
      "Training Accuracy: 0.7113333344459534\n",
      "Loss: 0.5382038354873657\n",
      "Training Accuracy: 0.7051666378974915\n",
      "Loss: 0.5427436232566833\n",
      "Training Accuracy: 0.7098333239555359\n",
      "Loss: 0.5440495014190674\n",
      "Training Accuracy: 0.7020000219345093\n",
      "Loss: 0.5395634174346924\n",
      "Training Accuracy: 0.7053333520889282\n",
      "Loss: 0.5437328815460205\n",
      "Training Accuracy: 0.7083333134651184\n",
      "Loss: 0.549691379070282\n",
      "Training Accuracy: 0.7055000066757202\n",
      "Loss: 0.5484655499458313\n",
      "Training Accuracy: 0.7041666507720947\n",
      "Loss: 0.5492469668388367\n",
      "Accuracy: 0.7088750600814819\n",
      "Training Accuracy: 0.7123333215713501\n",
      "Loss: 0.541599690914154\n",
      "Training Accuracy: 0.7064999938011169\n",
      "Loss: 0.5456797480583191\n",
      "Training Accuracy: 0.7111666798591614\n",
      "Loss: 0.5463007092475891\n",
      "Training Accuracy: 0.7083333134651184\n",
      "Loss: 0.5467343330383301\n",
      "Training Accuracy: 0.7081666588783264\n",
      "Loss: 0.5446469187736511\n",
      "Training Accuracy: 0.7053333520889282\n",
      "Loss: 0.5448525547981262\n",
      "Training Accuracy: 0.7005000114440918\n",
      "Loss: 0.5500760674476624\n",
      "Training Accuracy: 0.7108333110809326\n",
      "Loss: 0.5441096425056458\n",
      "Training Accuracy: 0.7095000147819519\n",
      "Loss: 0.5386143326759338\n"
     ]
    }
   ],
   "source": [
    "neg_size=1\n",
    "epoch=300\n",
    "mini_batch=g.number_of_nodes()\n",
    "for i in range(epoch):\n",
    "    loss = 0\n",
    "    h=model(features)\n",
    "    optimizer.zero_grad()\n",
    "#     for j in range(10):\n",
    "    pos_idx=torch.randint(0,len(click_data),(mini_batch,));\n",
    "    pos_itms=np.array(list(map(lambda x: iid2vid[click_data[x][1]],pos_idx)))\n",
    "    pos_usrs=np.array(list(map(lambda x: uid2vid[click_data[x][0]],pos_idx)))\n",
    "    neg_itms=sample_neg(pos_usrs,neg_size)\n",
    "    loss+=model.loss(h,pos_itms,pos_usrs,neg_itms)\n",
    "#     loss/=10\n",
    "    print(\"Loss: \", end =\"\")\n",
    "    print(float(loss))\n",
    "    if i %10==0:\n",
    "        pos_idx_test=torch.randint(0,len(click_data),(8000,));\n",
    "        pos_itms_test=np.array(list(map(lambda x: iid2vid[click_data[x][1]],pos_idx_test)))\n",
    "        pos_usrs_test=np.array(list(map(lambda x: uid2vid[click_data[x][0]],pos_idx_test)))\n",
    "        neg_itms_test=sample_neg(pos_usrs_test,neg_size)\n",
    "        print(\"Accuracy: \", end =\"\")\n",
    "        print(float(check_accuracy(h,pos_itms_test,pos_usrs_test,neg_itms_test)))\n",
    "        pickle.dump(model, open( \"./model_saved\", \"wb\" ))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Predicton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'h' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-1c0b453330ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0membedding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'h' is not defined"
     ]
    }
   ],
   "source": [
    "embedding=h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_clicks=genfromtxt(\"/home/cocoa/247Proj/underexpose_test/underexpose_test_qtime-0.csv\", delimiter=',',dtype=np.int32)\n",
    "for i in range(1,curr_phase+1):\n",
    "    test_clicks=np.concatenate((test_clicks, genfromtxt(f'/home/cocoa/247Proj/underexpose_test/underexpose_test_qtime-{i}.csv', delimiter=',',dtype=np.int32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_usr=test_clicks[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   11,    22,    44, ..., 35415, 35426, 35437], dtype=int32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_usr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([98764, 256])"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h[list(iid2vid.values()),:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_idx=g.out_degrees(np.arange(g.number_of_nodes())).topk(g.number_of_nodes()).indices\n",
    "popular=list()\n",
    "for i in top_idx:\n",
    "    if i in iid2vid.values():\n",
    "        popular.append(vid2iid[int(i)])\n",
    "        if len(popular)==50:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTopRec(usr_id):\n",
    "    #get item embedding\n",
    "    if usr_id in uid2vid:\n",
    "        dist=torch.norm (h[uid2vid[usr_id]]-h[list(iid2vid.values()),:],dim=1,p=None)\n",
    "        knn = dist.topk(50, largest=False)\n",
    "        return knn.indices+min(list(iid2vid.values()))\n",
    "    else:\n",
    "        return torch.Tensor(popular) #这里可以联系"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=dict()\n",
    "for i in target_usr:\n",
    "    result[i]=getTopRec(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(h, open( \"./embedding\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open( \"underexpose_submit-2.csv\",\"w\") as f:\n",
    "    for k,v in result.items():\n",
    "        f.write(str(k)+\",\")\n",
    "        f.write(\",\".join(list(map(lambda x:str(int(x)),v))))\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 47964,  77562,  50833,  92883,  63136, 100131,  96051,  72854,  64510,\n",
       "        104093,  44525,  55819,  66826,  86299, 120342,  44448,  92482,  94600,\n",
       "         42929, 126815,  66149,  36293,  39146,  37589,  58736,  43477,  39048,\n",
       "        125936, 103643,  40332,  87739,  67759,  58011, 111058,  36901,  97987,\n",
       "         39031,  40034,  88942,  98864,  52018,  78061,  66971,  66864,  42322,\n",
       "         39967,  48693,  38566,  62069,  52286], device='cuda:0')"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python38264bit96fe04a0e50340d0a2a9bf5a76471703"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
